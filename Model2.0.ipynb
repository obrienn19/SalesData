{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e046764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##README: this models runs perfectly well for all .csv files. All you need to do is (1.) clean the datasets,\n",
    "## (2.) change dataset names when concatenating, (3.) changethe data you are removing and withholding to do the actual prediction,\n",
    "## (4.) change the range length in the dummy indicator, (5.) figure out the data column(s) of your regressors and labels,\n",
    "## (6.) change the input dimensions in the input layer of the model\n",
    "\n",
    "#1. Import all data and clean data\n",
    "discretionarydata=pd.read_csv('Consumerdiscretionary.csv')\n",
    "discretionarydata1=pd.DataFrame(discretionarydata)\n",
    "discretionclean=discretionarydata1.drop(columns=['fyearq', 'fqtr', 'indfmt', 'consol', 'popsrc', 'datafmt', 'curcdq', 'datacqtr', 'datafqtr', 'costat','gsubind'])\n",
    "#print(discretionclean)\n",
    "\n",
    "discretionarydata2=pd.read_csv('Consumerstaples.csv')\n",
    "discretionarydata3=pd.DataFrame(discretionarydata2)\n",
    "consumerclean=discretionarydata3.drop(columns=['fyearq', 'fqtr', 'indfmt', 'consol', 'popsrc', 'datafmt', 'curcdq', 'datacqtr', 'datafqtr', 'costat','gsubind'])\n",
    "#print(consumerclean)\n",
    "\n",
    "discretionarydata4=pd.read_csv('Industrials.csv')\n",
    "discretionarydata5=pd.DataFrame(discretionarydata4)\n",
    "industrialsclean=discretionarydata5.drop(columns=['fyearq', 'fqtr', 'indfmt', 'consol', 'popsrc', 'datafmt', 'curcdq', 'datacqtr', 'datafqtr', 'costat','gsubind'])\n",
    "#print(industrialsclean)\n",
    "\n",
    "discretionarydata6=pd.read_csv('Energy.csv')\n",
    "discretionarydata7=pd.DataFrame(discretionarydata6)\n",
    "energyclean=discretionarydata7.drop(columns=['fyearq', 'fqtr', 'indfmt', 'consol', 'popsrc', 'datafmt', 'curcdq', 'datacqtr', 'datafqtr', 'costat','gsubind'])\n",
    "#print(energyclean)\n",
    "\n",
    "discretionarydata8=pd.read_csv('Healthcare.csv')\n",
    "discretionarydata9=pd.DataFrame(discretionarydata8)\n",
    "healthcareclean=discretionarydata9.drop(columns=['fyearq', 'fqtr', 'indfmt', 'consol', 'popsrc', 'datafmt', 'curcdq', 'datacqtr', 'datafqtr', 'costat','gsubind'])\n",
    "#print(healthcareclean)\n",
    "\n",
    "discretionarydata10=pd.read_csv('Financials.csv')\n",
    "discretionarydata11=pd.DataFrame(discretionarydata10)\n",
    "financialsclean=discretionarydata11.drop(columns=['fyearq', 'fqtr', 'indfmt', 'consol', 'popsrc', 'datafmt', 'curcdq', 'datacqtr', 'datafqtr', 'costat','gsubind'])\n",
    "#print(financialsclean)\n",
    "\n",
    "discretionarydata12=pd.read_csv('Communicationservices.csv')\n",
    "discretionarydata13=pd.DataFrame(discretionarydata12)\n",
    "communicationclean=discretionarydata13.drop(columns=['fyearq', 'fqtr', 'indfmt', 'consol', 'popsrc', 'datafmt', 'curcdq', 'datacqtr', 'datafqtr', 'costat','gsubind'])\n",
    "#print(communicationclean)\n",
    "\n",
    "\n",
    "#2. Combine all dataframes into one mega dataframe)\n",
    "megaset=pd.concat([discretionclean, consumerclean, industrialsclean, energyclean, healthcareclean, financialsclean, communicationclean])\n",
    "\n",
    "\n",
    "#3. Start creating test data by isolating a certain range of dates\n",
    "part1=megaset.loc[megaset['datadate'] == '3/31/2019']\n",
    "part2=megaset.loc[megaset['datadate'] == '6/30/2019']\n",
    "part3=megaset.loc[megaset['datadate'] == '9/30/2019']\n",
    "part4=megaset.loc[megaset['datadate'] == '12/31/2019']\n",
    "testx=pd.concat([part1,part2,part3,part4])\n",
    "\n",
    "#Clean dataset a bit more\n",
    "test1=testx.drop(['GVKEY'], axis=1)\n",
    "test2=test1.drop(['Unnamed: 18'], axis= 1)\n",
    "test3=test2.dropna()\n",
    "test4=test3.set_index([pd.Index(list(range(1095)))])\n",
    "a=range(1095)\n",
    "b=np.array(a)\n",
    "#Create dummy indicator for dates because original date format was in string form\n",
    "df=pd.DataFrame(data=b, columns=['Indicator'])\n",
    "test_data=pd.concat([test4, df], axis=1)\n",
    "print(test_data)\n",
    "\n",
    "#Start creating test data by dropping the previous range of dates\n",
    "trainx=megaset.set_index('datadate')\n",
    "train1=trainx.drop(['3/31/2019', '6/30/2019', '9/30/2019', '12/31/2019'], axis=0)\n",
    "#Clean dataset a bit more\n",
    "train2=train1.drop(['GVKEY'], axis=1)\n",
    "train3=train2.drop(['Unnamed: 18'], axis=1)\n",
    "train4=train3.dropna()\n",
    "train5=train4.reset_index()\n",
    "train6=train5.set_index([pd.Index(list(range(12339)))])\n",
    "c=range(12339)\n",
    "d=np.array(c)\n",
    "#Create dummy indicator for dates because original date format was in string form\n",
    "df1=pd.DataFrame(data=d, columns=['Indicator'])\n",
    "train_data=pd.concat([train6, df1], axis=1)\n",
    "print(train_data)\n",
    "\n",
    "#4. Isolate indicators from labels in the training set and the test set\n",
    "dates=train_data.index\n",
    "X=train_data.iloc[:,6]\n",
    "Y=train_data.iloc[:,4]\n",
    "Xi_test=test_data.iloc[:,6]\n",
    "yi_test=test_data.iloc[:,4]\n",
    "\n",
    "#5. Model function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, shuffle=True)\n",
    "model = Sequential()\n",
    "model.add(Dense(600,activation='relu',input_dim=1))\n",
    "\n",
    "model.add(Dense(300,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(150,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(75,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(7,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1,activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=128)\n",
    "\n",
    "#Results from model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('sqrt loss', np.sqrt(loss))\n",
    "print('standard deviation', train_data['revtq'].std())\n",
    "\n",
    "#Verify accuracy of results for training data\n",
    "predictions = model.predict(X)\n",
    "\n",
    "predictions_list = map(lambda x: x[0], predictions)\n",
    "print('predlist', predictions_list)\n",
    "predictions_series = pd.Series(predictions_list,index=dates)\n",
    "dates_series = pd.Series(dates)\n",
    "#for x in predictions:\n",
    " #   print('prediction', x[0])\n",
    "\n",
    "#Use previously generated model to generate labels for the test data\n",
    "Predicted_sales = model.predict(Xi_test)\n",
    "new_dates_series=pd.Series(Xi_test.index)\n",
    "new_predictions_list = map(lambda x: x[0], Predicted_sales)\n",
    "new_predictions_series = pd.Series(new_predictions_list,index=new_dates_series)\n",
    "\n",
    "#Export to CSV\n",
    "new_predictions_series.to_csv(\"predicted_saless.csv\",header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
